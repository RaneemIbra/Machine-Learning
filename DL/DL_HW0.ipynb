{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 0 - Deep Learning Winter 2024\n",
    "TA: Jerry Abu Ayoub\n",
    "\n",
    "Student 1: Raneem Ibraheem + 212920896\n",
    "\n",
    "Student 2: Aseel Nahas + ID\n",
    "\n",
    "**Submission Date and time: 28/11/2024 23:59**\n",
    "\n",
    "Submission is in pairs only, delayed submissions will result in a 5 point deduction each day for maximum of 3 days.\n",
    "\n",
    "in this assignment, you will experiment with some basic pytorch functions to manipulate data to understand the fundementals. read the text boxes and write your code in the code cells below to answer the questions and run the cells, when you finish all tasks, save the notebook and submit it in the course website.\n",
    "\n",
    "submission notebook should have all code cells already run with their respective outputs, submitting notebooks with unrunned cells will also result in point deductions.\n",
    "\n",
    "## Data Manipulation\n",
    "\n",
    "It is impossible to get anything done if we cannot manipulate data. Generally, there are two important things we need to do with data: (i) acquire it and (ii) process it once it is inside the computer. There is no point in acquiring data if we do not even know how to store it, so let's get our hands dirty first by playing with synthetic data. We will start by introducing the tensor,\n",
    "PyTorch's primary tool for storing and transforming data. If you have worked with NumPy before, you will notice that tensors are, by design, similar to NumPy's multi-dimensional array. Tensors support asynchronous computation on CPU, GPU and provide support for automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "In the cell below import torch and any other libraries you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors represent (possibly multi-dimensional) arrays of numerical values.\n",
    "The simplest object we can create is a vector. \n",
    "\n",
    "To start, use `arange` to create a row vector with 12 consecutive integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "x = torch.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the shape of the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "# We can get the tensor shape through the shape attribute.\n",
    "# write your code here\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use 'size' to print the shape of the tensor again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "# .shape is an alias for .size(), and was added to more closely match numpy\n",
    "# write your code here\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `reshape` function to change the shape of one (possibly multi-dimensional) array, to another that contains the same number of elements.\n",
    "For example, we can transform the shape of our line vector `x` to (3, 4), which contains the same values but interprets them as a matrix containing 3 rows and 4 columns. Note that although the shape has changed, the elements in `x` have not.\n",
    "\n",
    "reshape the vector into a 3x4 martix and print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "\n",
    "x = x.reshape(3,4)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping by manually specifying each of the dimensions can get annoying. Once we know one of the dimensions, why should we have to perform the division our selves to determine the other? For example, above, to get a matrix with 3 rows, we had to specify that it should have 4 columns (to account for the 12 elements). Fortunately, PyTorch can automatically work out one dimension given the other.\n",
    "We can invoke this capability by placing `-1` for the dimension that we would like PyTorch to automatically infer. \n",
    "\n",
    "In our case, instead of\n",
    "`x.reshape((3, 4))`, use `-1` to reshape the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "x.reshape(3,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a 2x3 float tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "floatTensor = torch.tensor([[1,2,3],[1,2,3]], dtype=torch.float32)\n",
    "floatTensor.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.Tensor() is just an alias to torch.FloatTensor() which is the default type of tensor, when no dtype is specified during tensor construction.\n",
    "\n",
    "From the torch for numpy users notes, it seems that torch.Tensor() is a drop-in replacement of numpy.empty()\n",
    "\n",
    "So, in essence torch.FloatTensor() and torch.empty() does the same job.\n",
    "\n",
    "The `empty` method just grabs some memory and hands us back a matrix without setting the values of any of its entries. This is very efficient but it means that the entries might take any arbitrary values, including very big ones! Typically, we'll want our matrices initialized either with ones, zeros, some known constant or numbers randomly sampled from a known distribution.\n",
    "\n",
    "Perhaps most often, we want an array of all zeros. \n",
    "\n",
    "Create a 2x3x4 multidimensional tensor with all entries as zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "zeroesTensor = torch.zeros((2,3,4))\n",
    "zeroesTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 2x3x4 tensor with all entries as ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "zeroesTensor = torch.ones((2,3,4))\n",
    "zeroesTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify the value of each element in the desired NDArray by supplying a Python list containing the numerical values.\n",
    "\n",
    "create a 2d tensor with the following lists as entries:\n",
    "[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 4, 3],\n",
       "        [1, 2, 3, 4],\n",
       "        [4, 3, 2, 1]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "\n",
    "y = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, we will want to randomly sample the values of each element in the tensor according to some known probability distribution. This is especially common when we intend to use the tensor as a parameter in a neural network. \n",
    "\n",
    "write a snippet that creates a tensor with a shape of (3,4). Each of its elements is randomly sampled in a normal distribution with zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.6524,  0.9491,  0.7102, -0.2237],\n",
       "        [-1.4534,  0.6495, -0.1748, -0.1699],\n",
       "        [ 0.0327,  0.9196, -0.7906,  1.2670]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "random_tensor = torch.randn(3, 4)\n",
    "random_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations\n",
    "\n",
    "Oftentimes, we want to apply functions to arrays. Some of the simplest and most useful functions are the element-wise functions. These operate by performing a single scalar operation on the corresponding elements of two arrays. We can create an element-wise function from any function that maps from the scalars to the scalars. In math notations we would denote such a function as $f: \\mathbb{R} \\rightarrow \\mathbb{R}$. Given any two vectors $\\mathbf{u}$ and $\\mathbf{v}$ *of the same shape*, and the function f,\n",
    "we can produce a vector $\\mathbf{c} = F(\\mathbf{u},\\mathbf{v})$ by setting $c_i \\gets f(u_i, v_i)$ for all $i$. Here, we produced the vector-valued $F: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ by *lifting* the scalar function to an element-wise vector operation. In PyTorch, the common standard arithmetic operators (+,-,/,\\*,\\*\\*) have all been *lifted* to element-wise operations for identically-shaped tensors of arbitrary shape. We can call element-wise operations on any two tensors of the same shape, including matrices.\n",
    "\n",
    "create a float tensor \"x\" that has: 1 2 4 8 and an another tensor \"y\" of all 2s\n",
    "\n",
    "compute and print the element wise operations of addition, subtraction, multiplication, division and exponentiation e^(for both tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x + y =  tensor([ 3.,  4.,  6., 10.])\n",
      "x - y =  tensor([-1.,  0.,  2.,  6.])\n",
      "x * y =  tensor([ 2.,  4.,  8., 16.])\n",
      "x / y =  tensor([0.5000, 1.0000, 2.0000, 4.0000])\n",
      "e^x =  tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])\n",
      "e^y =  tensor([7.3891, 7.3891, 7.3891, 7.3891])\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "\n",
    "x = torch.tensor([1,2,4,8], dtype=torch.float32)\n",
    "y = torch.tensor([2,2,2,2], dtype=torch.float32)\n",
    "print(\"x + y = \", x + y)\n",
    "print(\"x - y = \", x - y)\n",
    "print(\"x * y = \", x * y)\n",
    "print(\"x / y = \", x / y)\n",
    "print(\"e^x = \", torch.exp(x))\n",
    "print(\"e^y = \", torch.exp(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to computations by element, we can also perform matrix operations, like matrix multiplication using the `mm` or `matmul` function. Next, we will perform matrix multiplication of `x` and the transpose of `y`. We define `x` as a matrix of 3 rows and 4 columns, and `y` is transposed into a matrix of 4 rows and 3 columns. The two matrices are multiplied to obtain a matrix of 3 rows and 3 columns.\n",
    "\n",
    "define x as a 3x4 matrix with consecutive numbers from 0 to 11 and let y be a 2d array with the following lists as entries:\n",
    "\n",
    "[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]\n",
    "\n",
    "print the type of the tensors and print the multiplication of x and y transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of x: <class 'torch.Tensor'>\n",
      "Type of y: <class 'torch.Tensor'>\n",
      "x:\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "y transpose:\n",
      "tensor([[2., 1., 4.],\n",
      "        [1., 2., 3.],\n",
      "        [4., 3., 2.],\n",
      "        [3., 4., 1.]])\n",
      "Result of x * y_transpose:\n",
      "tensor([[ 18.,  20.,  10.],\n",
      "        [ 58.,  60.,  50.],\n",
      "        [ 98., 100.,  90.]])\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "x = torch.arange(12, dtype=torch.float32).reshape(3, 4)\n",
    "\n",
    "y = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]], dtype=torch.float32)\n",
    "y_transpose = y.T\n",
    "result = torch.matmul(x, y_transpose)\n",
    "\n",
    "print(\"Type of x:\", type(x))\n",
    "print(\"Type of y:\", type(y))\n",
    "print(\"x:\")\n",
    "print(x)\n",
    "print(\"y transpose:\")\n",
    "print(y_transpose)\n",
    "print(\"Result of x * y_transpose:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that torch.dot() behaves differently to np.dot(). There's been some discussion about what would be desirable here. Specifically, torch.dot() treats both x and y as 1D vectors (irrespective of their original shape) and computes their inner product. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also merge multiple tensors. For that, we need to tell the system along which dimension to merge. \n",
    "\n",
    "merge the previous two matrices along the row axis and column axis respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [ 2.,  1.,  4.,  3.],\n",
       "        [ 1.,  2.,  3.,  4.],\n",
       "        [ 4.,  3.,  2.,  1.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "rowMerge = torch.cat((x, y), dim=0)\n",
    "rowMerge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "columnMerge = torch.cat((x, y), dim=1)\n",
    "columnMerge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct a binary tensors via logical element wise equation, that is the entry is 1 if and only if the 2 elements are equal in the same position, otherwise its zero.\n",
    "\n",
    "use tensors x and y and compute the binary tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 1],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "binary_tensor = torch.eq(x, y).to(dtype=torch.int)\n",
    "binary_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing all the elements in the tensor yields an tensor with only one element.\n",
    "\n",
    "Sum all of x's elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "sum = torch.sum(x)\n",
    "sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast Mechanism\n",
    "\n",
    "In the above section, we saw how to perform operations on two tensors of the same shape. When their shapes differ, a broadcasting mechanism may be triggered analogous to NumPy: first, copy the elements appropriately so that the two tensors have the same shape, and then carry out operations by element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.],\n",
       "         [1.],\n",
       "         [2.]]),\n",
       " tensor([[0., 1.]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3, dtype=torch.float).reshape((3, 1))\n",
    "b = torch.arange(2, dtype=torch.float).reshape((1, 2))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `a` and `b` are (3x1) and (1x2) matrices respectively, their shapes do not match up if we want to add them. PyTorch addresses this by 'broadcasting' the entries of both matrices into a larger (3x2) matrix as follows: for matrix `a` it replicates the columns, for matrix `b` it replicates the rows before adding up both element-wise.\n",
    "\n",
    "add a and b and see for yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [1., 2.],\n",
       "        [2., 3.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "result = a + b\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing and Slicing\n",
    "\n",
    "Just like in any other Python array, elements in a tensor can be accessed by its index. In good Python tradition the first element has index 0 and ranges are specified to include the first but not the last element. \n",
    "\n",
    "use the \":' operand to select and print the first and second rows in x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3.],\n",
       "        [4., 5., 6., 7.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "firstTwo = x[:2]\n",
    "firstTwo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond reading, we can also write elements of a matrix like usual\n",
    "\n",
    "write 9 in position 1,2 in x and print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  9.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "x[1, 2] = 9\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use slicing to write 12 in the first and second rows of the matirx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12., 12., 12.],\n",
       "        [12., 12., 12., 12.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "x[:2, :] = 12\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Memory\n",
    "\n",
    "In the previous example, every time we ran an operation, we allocated new memory to host its results. For example, if we write `y = x + y`, we will dereference the matrix that `y` used to point to and instead point it at the newly allocated memory. In the following example we demonstrate this with Python's `id()` function, which gives us the exact address of the referenced object in memory. After running `y = y + x`, we will find that `id(y)` points to a different location. That is because Python first evaluates `y + x`, allocating new memory for the result and then subsequently redirects `y` to point at this new location in memory.\n",
    "\n",
    "use id() to check if the first y tensor is in the same memory of the resul y tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(y)\n",
    "y = y + x\n",
    "id(y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might be undesirable for two reasons. First, we do not want to run around allocating memory unnecessarily all the time. In machine learning, we might have hundreds of megabytes of parameters and update all of them multiple times per second. Typically, we will want to perform these updates *in place*. Second, we might point at the same parameters from multiple variables. If we do not update in place, this could cause a memory leak, making it possible for us to inadvertently reference stale parameters.\n",
    "\n",
    "Fortunately, performing in-place operations in PyTorch is easy. We can assign the result of an operation to a previously allocated array with slice notation, e.g., `y[:] = <expression>`. To illustrate the behavior, we first clone the shape of a matrix using `zeros_like` to allocate a block of 0 entries.\n",
    "\n",
    "allocate a block of zeroes into a tensor named z, print the id of z, perfomr the addition of x and y into z and print the id of z again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID before:  2524074414464\n",
      "ID after:  2524074414464\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)\n",
    "y = torch.tensor([[7, 8, 9], [10, 11, 12]], dtype=torch.float32)\n",
    "z = torch.zeros_like(x)\n",
    "print(\"ID before: \", id(z))\n",
    "z[:] = x + y\n",
    "print(\"ID after: \", id(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Transformation of PyTorch and NumPy\n",
    "\n",
    "Converting PyTorch Tensors to and from NumPy Arrays is easy. The converted arrays do *not* share memory. This minor inconvenience is actually quite important: when you perform operations on the CPU or one of the GPUs, you do not want PyTorch having to wait whether NumPy might want to be doing something else with the same chunk of memory. `.tensor` and `.numpy` do the trick.\n",
    "\n",
    "convert \"a\" to a torch tensor and put in \"b\" then print b's type to ensure it's a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Type of b:  <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "a = x.numpy()\n",
    "print(type(a))\n",
    "b = torch.tensor(a)\n",
    "print(\"Type of b: \", type(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of Latex and Mathematics notations, usage of Autograd in pytorch\n",
    "\n",
    "Use Latex to write down the 2 variable bell function (Recall from calculus 2), write it in understandable tidy mathematical formation (the most simple one).\n",
    "\n",
    "-- your function in Latex should appear here --\n",
    "<font color='red'> $$ f(x,y)=e^{-(x^2 + y^2)} $$ </font>\n",
    "\n",
    "Use Latex again to write the gradient of the function, it should appear in a nice understandable mathematical format.\n",
    "\n",
    "-- your function's gradient in Latex should appear here --\n",
    "<font color='red'> $$ ∇f(x,y)=(\\frac{∂x}{∂f}, \\frac{∂y}{∂f}) = (−2xe^{−(x2+y2)} ,−2ye^{−(x 2 +y 2 )})$$</font>\n",
    "\n",
    "write a piece code in the cell below using pytorch to do the following:\n",
    "\n",
    "- define the bell function\n",
    "- print it's value at (0,0) and (1,1)\n",
    "- print the gradient's value at those points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point (0.0, 0.0)\n",
      "function value: 1.0\n",
      "gradient value: (df/dx=-0.0, df/dy=-0.0)\n",
      "\n",
      "point (1.0, 1.0)\n",
      "function value: 0.1353352814912796\n",
      "gradient value: (df/dx=-0.2706705629825592, df/dy=-0.2706705629825592)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "def bell_function(x, y):\n",
    "    return torch.exp(-(x**2 + y**2))\n",
    "\n",
    "points = [(0.0, 0.0), (1.0, 1.0)]\n",
    "for x_val, y_val in points:\n",
    "    x = torch.tensor(x_val, requires_grad=True)\n",
    "    y = torch.tensor(y_val, requires_grad=True)\n",
    "    f = bell_function(x, y)\n",
    "    f.backward()\n",
    "\n",
    "    print(f\"point ({x.item()}, {y.item()})\")\n",
    "    print(f\"function value: {f.item()}\")\n",
    "    print(f\"gradient value: (df/dx={x.grad.item()}, df/dy={y.grad.item()})\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
