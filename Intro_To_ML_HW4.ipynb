{"cells":[{"cell_type":"markdown","metadata":{"id":"iBHBci_i2IgA"},"source":["# Introducrtion to Machine Learning: Assignment #4\n","## Submission date: 31\\07\\2024, 23:55.\n","### Topics:\n","- Ensemble methods\n","- AdaBoost\n","- PCA\n","- LDA\n","- K means clustering"]},{"cell_type":"markdown","metadata":{"id":"NGo4MrnG2NXa"},"source":["Submitted by:\n","\n"," Raneem Ibraheem 212920896\n","\n"," Selan Abu Saleh 212111439"]},{"cell_type":"markdown","metadata":{"id":"eietFcHy2Kr2"},"source":["**Assignment Instruction:**\n","\n","· Submissions in pairs only.\n","\n","· Try to keep the code as clean, concise, and short as possible\n","\n","· If you wish to work in your IDE, you can, but you **must**,  insert the script back to the matching cells of the notebook and run the code. <br/>Only the notebook will be submitted in moodle (in `.ipynb` format).\n","\n","· <font color='red'>Please write your answers to question in red</font>.\n","\n","**Important:** All plots, results and outputs should be included in the notebook as the cells' outputs (run all cells and do not clear the output). <br/>\n","\n","**Important:** Your submission must be entirely your own. Any attempts of plagiarism (including ChatGPT) will lead to grade 0 and disciplinary actions.\n"]},{"cell_type":"markdown","metadata":{"id":"Kh7ZuCb6r9Fs"},"source":["## Question 1 - Bagging\n","In HW3, you helped Charles Darvin with regression of abalone problem and now, you will try combining multiple regression models instead of just one, hopefully for a better result."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"tDcWtpOxuaqd"},"outputs":[],"source":["# import libraries\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","\n","scaler = StandardScaler()"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"K9dL92Wbuhb5"},"outputs":[],"source":["# load the data, transform it\n","\n","df = pd.read_csv('https://sharon.srworkspace.com/ml/datasets/hw3/abalone.csv', header=None)\n","data = df.to_numpy()\n","\n","train, test = train_test_split(data, test_size=0.2, random_state=21)\n","\n","X_train, y_train = train[:,:-1], train[:, -1]\n","X_test, y_test = test[:,:-1], test[:, -1]\n","\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"7Zs4Gjueu0CY"},"source":["Your task is to complete the following bagging model. Specifically:\n","- fit(self, data, targets) - train n_estimators regressors, each with data in size n=len(data) that is drawn from the original data, with repititions.\n","- predict(self, test) - predict the result for all the regressors as learned.\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"mfx59vQd0J8m"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","\n","class LinRegCombiner():\n","    def __init__(self, n_estimators):\n","        self.n_estimators = n_estimators\n","\n","    def fit(self, data, targets):\n","        self.regressors = []\n","        samples_num = data.shape[0]\n","\n","        for _ in range(self.n_estimators):\n","            # Implement here\n","            indecis = np.random.choice(samples_num, samples_num, replace=True)\n","            sample_data  = data[indecis]\n","            sample_targets = targets[indecis]\n","\n","            regression = LinearRegression()\n","            regression.fit(sample_data, sample_targets)\n","            self.regressors.append(regression)\n","\n","\n","    def predict(self, test):\n","        preds = np.zeros((self.n_estimators, test.shape[0]))\n","        # Implement here\n","        for i, regressor in enumerate(self.regressors):\n","            preds[i] = regressor.predict(test)\n","            \n","        return preds.mean(axis=0)\n","\n","    def score(self, test, targets):\n","        temp = self.predict(test)\n","        return np.mean((targets - temp) ** 2)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"P3EVI4T10IWj"},"outputs":[{"name":"stdout","output_type":"stream","text":["MSE train = 4.80914043779686\n","MSE test = 5.380999372095517\n"]}],"source":["# Run for 100 estimators\n","\n","obj = LinRegCombiner(n_estimators=100)\n","obj.fit(X_train, y_train)\n","\n","mse = obj.score(X_train, y_train)\n","print(f'MSE train = {mse}')\n","\n","mse = obj.score(X_test, y_test)\n","print(f'MSE test = {mse}')"]},{"cell_type":"markdown","metadata":{"id":"8KxmUKpW34YX"},"source":["Now, implement the same but using ridge regression"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"HSCAHyo4375e"},"outputs":[],"source":["from sklearn.linear_model import Ridge\n","from sklearn.base import BaseEstimator, RegressorMixin\n","\n","class RidgeLinRegCombiner(BaseEstimator, RegressorMixin):\n","    def __init__(self, n_estimators, alpha=1.0):\n","        self.n_estimators = n_estimators\n","        self.alpha = alpha\n","\n","    def fit(self, data, targets):\n","        self.regressors = []\n","        samples_num = data.shape[0]\n","        \n","        for _ in range(self.n_estimators):\n","            # Implement here\n","            indecis = np.random.choice(samples_num, samples_num, replace=True)\n","            sample_data  = data[indecis]\n","            sample_targets = targets[indecis]\n","\n","            regression = Ridge(alpha=self.alpha)\n","            regression.fit(sample_data, sample_targets)\n","            self.regressors.append(regression)\n","\n","    def predict(self, test):\n","        preds = np.zeros((self.n_estimators, test.shape[0]))\n","        # Implement here\n","        for i, regressor in enumerate(self.regressors):\n","            preds[i] = regressor.predict(test)\n","\n","        return preds.mean(axis=0)\n","\n","    def score(self, test, targets):\n","        temp = self.predict(test)\n","        return np.mean((targets - temp) ** 2)"]},{"cell_type":"markdown","metadata":{"id":"WRgpTlHh4C-X"},"source":["Tune the hyperparameters for RidgeLinRegCombiner."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aaaGgQip4pg4"},"outputs":[],"source":["# Implement here"]},{"cell_type":"markdown","metadata":{"id":"RpYoylPfK-A0"},"source":["Print both MSEs on train and test.\n","Which model is better for our problem? <br/>\n","<font color='red'>Write your answer here and explain it</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kvg4j7LzKJHb"},"outputs":[],"source":["# Run for n estimators and alpha you found\n","\n","obj = RidgeLinRegCombiner(n_estimators=n, alpha=alpha)\n","obj.fit(X_train, y_train)\n","\n","mse = obj.score(X_train, y_train)\n","print(f'MSE train = {mse}')\n","\n","mse = obj.score(X_test, y_test)\n","print(f'MSE test = {mse}')"]},{"cell_type":"markdown","metadata":{"id":"wqPk-EK5tBJT"},"source":["## Question 2 - Clustering\n","\n","We learned in the tutorials about partitional clustering and specifically – k means algorithm. <br/>\n","In this question you will implement it and see some nice applications."]},{"cell_type":"markdown","metadata":{"id":"KTd61ral4Ju3"},"source":["import libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"NA919a0U4MFo"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"7LiNstqG3peu"},"source":["Complete the missing implementation of Kmeans. Since there are k clusters, we will label each point with {0,..,k-1}."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8ACpogUs4ux"},"outputs":[],"source":["class Kmeans:\n","\n","\tdef __init__(self, n_clusters, max_iter=100, random_state=123):\n","\t\tself.n_clusters = n_clusters\n","\t\tself.max_iter = max_iter\n","\t\tself.random_state = random_state\n","\n","\tdef initialize_centroids(self, X):\n","\t\tnp.random.RandomState(self.random_state)\n","\t\trandom_idx = np.random.permutation(X.shape[0])\n","\t\tcentroids = X[random_idx[:self.n_clusters]]\n","\t\treturn centroids\n","\n","\tdef reassign_centroids(self, X, labels):\n","\t\tcentroids = np.zeros((self.n_clusters, X.shape[1]))\n","\t\t# Implement here\n","\t\treturn centroids\n","\n","\tdef compute_distance(self, X, centroids):\n","\t\tdistance = np.zeros((X.shape[0], self.n_clusters))\n","\t\tfor k in range(self.n_clusters):\n","\t\t\trow_norm = np.linalg.norm(X - centroids[k, :], axis=1)\n","\t\t\tdistance[:, k] = np.square(row_norm)\n","\t\treturn distance\n","\n","\tdef find_closest_cluster(self, distance):\n","\t\treturn np.argmin(distance, axis=1)\n","\n","\tdef compute_sse(self, X, labels, centroids):\n","\t\tdistance = np.zeros(X.shape[0])\n","\t\tfor k in range(self.n_clusters):\n","\t\t\tdistance[labels == k] = np.linalg.norm(X[labels == k] - centroids[k], axis=1)\n","\t\treturn np.sum(np.square(distance))\n","\n","\tdef fit(self, X):\n","\t\tself.centroids = self.initialize_centroids(X)\n","\t\tfor i in range(self.max_iter):\n","\t\t\told_centroids = self.centroids\n","\t\t\t# For each point, calculate distance to all k clustes.\n","\t\t\tself.labels =\t# Assign the labels with closest distance' cluster.\n","\t\t\tself.centroids = # Update the centroids\n","\t\t\tif np.all(old_centroids == self.centroids):\n","\t\t\t\tbreak\n","\t\tself.error = self.compute_sse(X, self.labels, self.centroids)\n","\n","\tdef predict(self, X):\n","\t\tdistance = self.compute_distance(X, self.centroids)\n","\t\treturn self.find_closest_cluster(distance)"]},{"cell_type":"markdown","metadata":{"id":"01SMbF1T6UVM"},"source":["Load exams data, convert to numpy and plot it"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GVLM73H9vb69"},"outputs":[],"source":["db = pd.read_csv('https://sharon.srworkspace.com/ml/datasets/hw4/exams.csv', header=None).to_numpy()\n","data, labels = db[:,:-1], db[:,-1]\n","\n","plt.scatter(data[:, 0], data[:, 1])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"i3yHa5ap6k3W"},"source":["We are going to divide the data into 2 clusters. <br/>\n","Define Kmeans object and fit the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DV8XrX6AvdH-"},"outputs":[],"source":["clust = Kmeans(n_clusters=2)\n","clust.fit(data)\n","\n","# This code plots the clustered data with centroids\n","labels = clust.labels\n","centroids = clust.centroids\n","\n","c0 = data[labels == 0]\n","c1 = data[labels == 1]\n","\n","plt.scatter(c0[:,0], c0[:,1], c='green', label='cluster 1')\n","plt.scatter(c1[:,0], c1[:,1], c='blue', label='cluster 2')\n","plt.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=200, c='black', label='centroid')\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Vn-tPolL8564"},"source":["Use the Elbow Method to choose another number of centroids between 1-10. <br/>\n","<font color='red'>Explain your choice</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-3ebxS6y87C9"},"outputs":[],"source":["sse = []\n","list_k = list(range(1, 11))\n","\n","for k in list_k:\n","  sse.append(error_of_current_clustering)\n","\n","'''Plot sse against k'''\n","plt.figure(figsize=(6, 6))\n","plt.plot(list_k, sse, '-o')\n","plt.xlabel(r'Number of clusters *k*')\n","plt.ylabel('Sum of squared distance')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"NbZZOx8W8BTS"},"source":["Apply clustering with the selected k"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xR-GMy467BdZ"},"outputs":[],"source":["# Implement here"]},{"cell_type":"markdown","metadata":{"id":"qbVNVuZo8ObI"},"source":["Now, you will compress some image using k-means. <br/>\n","Here, you are given image from size 400x600x3. The last parameter is the number of channels. 3 channels means that the image is colored (unlike 1 in, which is grayscale). <br/>\n","Our goal is to reduce the number of colors to 20 and represent (compress) the photo using those 20 colors only. <br/>\n","\n","Motivation: the original image requires 400x600x3x8 bits, while the new image will require only 400x600x5 + 20x24 bits, almost 5 times smaller!<br/>\n","To really do this, we will take the image and treat every pixel as a data point, where each data point is in 3d space (r,g,b). Then, we cluster into 20 centroids, and we assign each pixel to a centroid. This will allow us to represent the image using only 20 colors.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"oFBFAZFj-VkY"},"outputs":[],"source":["#@title helper function\n","\n","import urllib.request\n","\n","def read_image(url):\n","    req = urllib.request.urlopen(url)\n","    arr = np.asarray(bytearray(req.read()), dtype=np.uint8)\n","    return cv2.imdecode(arr, -1)"]},{"cell_type":"markdown","metadata":{"id":"7Tfm_EwY9QWy"},"source":["Complete the missing code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZjYkvr8J9Bul"},"outputs":[],"source":["from sklearn.cluster import KMeans\n","import cv2\n","\n","img = read_image('https://sharon.srworkspace.com/ml/datasets/hw4/image.jpg')\n","img_size = img.shape\n","\n","# Reshape it to be 2-dimension\n","X = img.reshape(img_size[0] * img_size[1], img_size[2])\t\t# Turn hxwx3 into (h*w)x3\n","\n","# Run the Kmeans algorithm\n","km = KMeans(n_clusters=20)\n","km.fit(X)\n","\n","'''\n","The km has the following properties:\n","(*) km.labels_ is an array size (pixels, 20), will give each pixel its class from 20 classes (values are between 0-19)\n","(*) km.cluster_centers_ is an array size 20x3, where the ith row represents the color value for the ith label.\n","\tFor example, cluster_centers_[0] = [r,g,b], the first center.\n","'''\n","\n","# Use the centroids to compress the image\n","img_compressed = # Use cluster_centers_ and labels_\n","img_compressed = np.clip(img_compressed.astype('uint8'), 0, 255)\n","\n","# Reshape X_recovered to have the same dimension as the original image 128 * 128 * 3'''\n","img_compressed = img_compressed.reshape(img_size[0], img_size[1], img_size[2])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rAvDpiqO_S4B"},"outputs":[],"source":["\n","# Plot the original and the compressed image next to each other'''\n","plt.figure(figsize = (12, 4))\n","\n","plt.subplot(1, 2, 1)\n","plt.imshow(img)\n","plt.title(\"Original Image\")\n","\n","plt.subplot(1, 2, 2)\n","plt.imshow(img_compressed)\n","plt.title(f'Compressed Image with {km.n_clusters} colors')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"d_iKlHnjsiBj"},"source":["## load smiling dataset\n","\n","In the third question, we will deal with the Smiling-face dataset, which determines if a person is smiling or not. You will try several models and hope to get good results<br/>\n","Your task is: run the following section and make sure your understand what's going on."]},{"cell_type":"markdown","metadata":{"id":"Kq3cRhYC6AaA"},"source":["Go to your <a href=\"https://www.kaggle.com/\">Kaggle</a> account and under the settings, generate new API token. <br/>\n","This will give you the json file, which you will upload here."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XsC3aagYU8Bg"},"outputs":[],"source":["# The script expects you to upload JSON file to it!\n","\n","! pip install -q kaggle\n","from google.colab import files\n","files.upload()\n","! mkdir ~/.kaggle\n","! cp kaggle.json ~/.kaggle/\n","! chmod 600 ~/.kaggle/kaggle.json\n","! kaggle datasets list\n","! kaggle datasets download chazzer/smiling-or-not-face-data\n","! unzip -q smiling-or-not-face-data.zip -d data"]},{"cell_type":"markdown","metadata":{"id":"lB0zaNLzslMW"},"source":["import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V1j2in2zXhT0"},"outputs":[],"source":["import numpy as np\n","from matplotlib import pyplot as plt\n","import os\n","import cv2\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"1UTtYOlw9eDa"},"source":["process the images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hvQvtLOAXk8q"},"outputs":[],"source":["def proccess_data(folder):\n","\timage_arrays = []\n","\tfor filename in os.listdir(folder):\n","\t\tfile_path = os.path.join(folder, filename)\n","\t\timage = cv2.imread(file_path)\n","\t\tgray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\t\timage_arrays.append(gray_image)\n","\treturn np.array(image_arrays)\n","\n","smile = proccess_data('./data/smile')\n","non_smile = proccess_data('./data/non_smile')\n","\n","dataset = np.vstack((smile,non_smile))\n","dataset = dataset / 255\n","\n","labels = [0] * len(smile) + [1] * len(non_smile)\n","labels = np.array(labels)"]},{"cell_type":"markdown","metadata":{"id":"1nW9k369dhYT"},"source":["display smiling and non-smiling image."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QfbqqhCadGRX"},"outputs":[],"source":["plt.subplot(121)\n","plt.title(\"Smile\")\n","plt.imshow(smile[0], cmap='gray')\n","\n","plt.subplot(122)\n","plt.title(\"Not smile\")\n","plt.imshow(non_smile[0], cmap='gray')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"rEVo_5-k990Z"},"source":["print the smiling and non-smiling data + the united dataset along with labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4hCqccgRZUzT"},"outputs":[],"source":["print(f'smile array size is (images, height, width)={smile.shape}')\n","print(f'non smile array size is (images, height, width)={non_smile.shape}')\n","print()\n","print(f'dataset array size is {dataset.shape}')\n","print(f'labels array size is {labels.shape}')"]},{"cell_type":"markdown","metadata":{"id":"nw3w7aGusyDN"},"source":["Prepear train and test datasets, print their structure. Since you have to deal with 1d features, we flatten the squared image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dhyWZgWXbE4I"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test = train_test_split(dataset, labels, test_size = 0.2, stratify=labels, random_state=42)\n","\n","print(f'train size is {x_train.shape} and labels size is {y_train.shape}')\n","print(f'test size is {x_test.shape} and labels size is {y_test.shape}')\n","print()\n","\n","x_train_flatten = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2])\n","x_test_flatten = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2])\n","\n","print(f'flattened train size is {x_train_flatten.shape} ')\n","print(f'flattened test size is {x_test_flatten.shape}')"]},{"cell_type":"markdown","metadata":{"id":"scLdbLSYskMr"},"source":["## Question 3 - PCA, LDA\n","\n","You will try to classify the smiling faces dataset using feature reduction and KNN (since there are 4096 features!). Than you will compare it to LDA"]},{"cell_type":"markdown","metadata":{"id":"u0KjWT27q-kd"},"source":["import libraries"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"1tV-D9usq8eo"},"outputs":[],"source":["from sklearn.model_selection import cross_val_score\n","from sklearn.neighbors import KNeighborsClassifier"]},{"cell_type":"markdown","metadata":{"id":"HVPuhAjX-k9f"},"source":["Implement PCA to reduce the dimension of the images from 4096=64x64 to 81=9x9. For time effciency, DONT use any loops here.\n","\n","Hint: Implement inverse_transform to recover the original vector from the compressed one. <br/>\n","Hint: When dealing with symmetric matrix, you can call eigh instead of eig function of numpy, its much faster."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"edDFB2MLcU4Q"},"outputs":[],"source":["def PCA_train(data, k):\n","\t# Implement here\n","\t# Download data to k dimensions\n","\tmean = np.mean(data, axis=0)\n","\tproccessed_data = data - mean\n","\tcov_mat = np.cov(proccessed_data, rowvar= False)\n","\teigenVal, eigenVec = np.linalg.eigh(cov_mat)\n","\tsorted_i = np.argsort(eigenVal)[::-1]\n","\teigenVal = eigenVal[sorted_i]\n","\teigenVec = eigenVec[:,sorted_i]\n","\tlargest_K_Vec = eigenVec[:,:k]\n","\tx_train_new = np.dot(proccessed_data, largest_K_Vec)\n","\treturn x_train_new, mean, largest_K_Vec\n","def PCA_test(test, mu, E):\n","\t# Implement here\n","\tprocessed_test = test - mu\n","\treturn np.dot(processed_test, E)\n","\n","def recover_PCA(data, mu, E):\n","\t# Implement here\n","\treturn np.dot(data, E.T) + mu"]},{"cell_type":"markdown","metadata":{"id":"8XGKa-V4ARsq"},"source":["Apply the PCA. <br/>\n","Make sure you fit the PCA model only to the training set (but apply it to both training and test sets). <br/>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_jdtyXCLeGlx"},"outputs":[],"source":["x_train_new, mu, eig = # Implement here\n","x_test_new = # Implement here"]},{"cell_type":"markdown","metadata":{"id":"4OroYh4NAd6A"},"source":["Pick another random image and show the result of applying PCA to it, and then try to recover the whole size again."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"US1NS8Mdf4ip"},"outputs":[],"source":["plt.subplot(131)\n","plt.title(\"Original Image\")\n","plt.imshow(None, cmap='gray')\n","\n","plt.subplot(132)\n","plt.title(\"Image in lower dimension\")\n","plt.imshow(None, cmap='gray')\n","\n","plt.subplot(133)\n","plt.title(\"Recovered Image\")\n","plt.imshow(None, cmap='gray')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"HkI1iur3BDF4"},"source":["Before training the model, use EIG_CDF, that given eigenvalues, draws a CDF of them like here:<br/><br/>\n","\n","![Picture1.jpg](https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcS3mOZk1x4X3ap9nuMnst5W5pMgOXF8r3Tmx1QcFX9mba_lleuB)\n","\n","As seen in the tutorials, we use them to see how much \"energy\" we preserve from the data. Use this to choose optimal dimension to reduce into, such the preserves 95% of the energy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k-zaqczg_VGr"},"outputs":[],"source":["def EIG_CDF(eig_list):\n","\tsorted_eigenvalues = np.sort(eig_list)[::-1]\n","\n","\teigenvalues_cumsum = np.cumsum(sorted_eigenvalues)\n","\n","\teigenvalues_cumsum_normalized = eigenvalues_cumsum / eigenvalues_cumsum[-1]\n","\tamount = # Implement here\n","\n","\tplt.plot(np.arange(1, len(sorted_eigenvalues)+1), eigenvalues_cumsum_normalized)\n","\tplt.xlabel('Principal Component')\n","\tplt.ylabel('Cumulative Proportion of Variance')\n","\tplt.title(f'CDF of Eigenvalues - {amount} eigs preserves 95% of enetry')\n","\tplt.show()\n","\n","# Call to EIG_CDF"]},{"cell_type":"markdown","metadata":{"id":"O6nbAp0mBuqM"},"source":["For the same image as before, show the result of applying PCA to it and recovering.<br/>\n","Is the result better? What is different from 81 dimensions? <br/>\n","<font color='red'>Write here your answer and explain it</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2m7DAhXOCerg"},"outputs":[],"source":["plt.subplot(131)\n","plt.title(\"Original Image\")\n","plt.imshow(None, cmap='gray')\n","\n","plt.subplot(132)\n","plt.title(\"Image in lower dimension\")\n","plt.imshow(None, cmap='gray')\n","\n","plt.subplot(133)\n","plt.title(\"Recovered Image\")\n","plt.imshow(None, cmap='gray')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"bqaVkkL6s1rE"},"source":["Now, you are ready to train the model. Use KNN, tune the best k using cross_val_score (with sklearn)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lNlJMtGYnh6T"},"outputs":[],"source":["# Implement here\n","\n","plt.figure(figsize=(14,5))\n","plt.plot(ks, accs)\n","plt.xlabel('k')\n","plt.xticks(ks)\n","plt.ylabel('avg accuracy')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"rWqFTbTcC8d1"},"source":["Print the accuracy of your model on the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FvKlB4K3e7sD"},"outputs":[],"source":["# Implement here\n","print(f'acc on test is {acc}')"]},{"cell_type":"markdown","metadata":{"id":"CVJW2S-vtDB3"},"source":["Answer the following sum-up questions: <br/>\n","- What pre-proccessing actions were done on the data?\n","- Should we apply Standard Scaler? Why?\n","- Suggest one idea for improvement (rather than LDA)\n","\n","<font color='red'>Write here your answers, with explainations</font>\n"]},{"cell_type":"markdown","metadata":{"id":"fNIenItVpEYt"},"source":["For the second part, we will use LDA on the data <b>before</b> PCA. <br/>\n","Use the model of LinearDiscriminantAnalysis from Sklearn, train the data and print the accuracy test using KNN. <br/>\n","Use the best k you found earlier."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q0JrHVlvpbhj"},"outputs":[],"source":["# Implement here"]},{"cell_type":"markdown","metadata":{"id":"hjtGXZ6Kpcu8"},"source":["Now, repeat the same as above, but using the data <b>after</b> PCA and using the same k.\n","- Was the combination of PCA and LDA helpful more than LDA alone?\n","- Comparing LDA alone vs PCA alone (as dimensionality reduction), which one was better to this problem? Justify.\n","\n","<font color='red'>Write your answers here and explain them.</font>"]},{"cell_type":"markdown","metadata":{"id":"DcjNI7QE9j70"},"source":["## Question 4 - Adaboost\n","See attached pdf in moodle assignment!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"E2rCBEixpHac"},"source":["## Question 5 - Kernel PCA - 10 pts bonus\n","See attached pdf in moodle assignment! <br/>\n","Here you will implement the parts that are relevant for that question"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"or0URrvxvBnz"},"outputs":[],"source":["# Implement here everything you need"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM1mEU/byF69UVDYlRU1FVc","collapsed_sections":["Kh7ZuCb6r9Fs","wqPk-EK5tBJT","d_iKlHnjsiBj","scLdbLSYskMr","E2rCBEixpHac"],"mount_file_id":"1Kr1hAcsz3nQe5ce3vdzAXXyH9YeUk5YY","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
